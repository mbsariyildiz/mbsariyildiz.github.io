<!DOCTYPE HTML>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Mert Bulent Sariyildiz's Personal Website</title>

    <meta name="author" content="Mert Bulent Sariyildiz">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/png" href="profile/website-icon.png">
</head>

<body>
    <table
        style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">

                    <!-- BIO -->
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr style="padding:0px">
                                <td style="padding:2.5%;width:75%;vertical-align:middle">
                                    <p style="text-align:center">
                                        <name>Mert Bulent Sariyildiz</name>
                                    </p>
                                    <p>
                                        I am a PhD student at <a href="https://europe.naverlabs.com">NAVER LABS
                                            Europe</a>
                                        and Inria Grenoble (<a href="https://team.inria.fr/thoth">the THOTH Team</a>) in
                                        France,
                                        working with my amazing supervisors
                                        <a href="https://www.skamalas.com">Yannis Kalantidis</a>,
                                        <a href="https://dlarlus.github.io">Diane Larlus</a> and
                                        <a href="https://thoth.inrialpes.fr/people/alahari">Karteek Alahari</a>.
                                        My PhD focuses on learning general-purpose visual representations from images.
                                    </p>
                                    <p>
                                        I received my M.Sc. degree from <a href="https://w3.cs.bilkent.edu.tr/en">the
                                            Computer Engineering Department</a> at Bilkent University in Turkey.
                                        During my master, I worked with <a
                                            href="https://user.ceng.metu.edu.tr/~gcinbis">Gokberk Cinbis</a> on learning
                                        data-efficient visual embedding models.
                                        I received my B.Sc. from <a href="https://eem.eskisehir.edu.tr">the Electrical
                                            and Electronics Engineering Department</a> at Anadolu (now Eskisehir
                                        Technical) University in Turkey.
                                    </p>
                                    <p style="text-align:center">
                                        <a href="mailto:mbsariyildiz@gmail.com">Email</a> &nbsp/&nbsp
                                        <a href="data/MertBulentSariyildiz-CV.pdf">CV</a> &nbsp/&nbsp
                                        <a href="https://scholar.google.com/citations?user=9vpQ9tIAAAAJ&hl=en">Google
                                            Scholar</a> &nbsp/&nbsp
                                        <a href="https://twitter.com/mbsariyildiz">Twitter</a> &nbsp/&nbsp
                                        <a href="https://github.com/mbsariyildiz">Github</a>
                                    </p>
                                </td>
                                <td style="padding:2.5%;width:25%;max-width:25%">
                                    <img style="max-width:100%;height:auto;" alt="profile photo"
                                        src="profile/MertBulentSariyildiz-pic.png">
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <!-- News-->
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>News</heading>
                                    <p>
                                    <ul>
                                        <li>
                                            [<em>2022-06</em>] -
                                            Our work <a href="#row-22trex">Improving the Generalization of
                                                Supervised Models</a> is on <a
                                                href="https://arxiv.org/abs/2206.15369">ArXiv</a>.<br>
                                            Pretrained weights for t-ReX and t-ReX* are available on <a
                                                href="https://europe.naverlabs.com/research/computer-vision/improving-the-generalization-of-supervised-models/#pretrained_models">our
                                                project
                                                webpage</a>.
                                        </li>
                                        <li>
                                            [<em>2021-08</em>] -
                                            <a href="#row-21cog">The ImageNet-CoG benchmark</a> is accepted to <a
                                                href="http://iccv2021.thecvf.com/home">ICCV 2021</a>!<br>
                                            We have a newer version of the manuscript, and our code is out!
                                        </li>
                                        <li>
                                            [<em>2021-07</em>] -
                                            I'm selected as an <a href="http://cvpr2021.thecvf.com/node/184">outstanding
                                                reviewer at CVPR 2021</a>.
                                        </li>
                                        <li>
                                            [<em>2020-12</em>] -
                                            The pre-print of the ImageNet-CoG Benchmark is on <a
                                                href="https://arxiv.org/abs/2012.05649">arXiv</a>.
                                        </li>
                                        <li>
                                            [<em>2020-09</em>] -
                                            I officially started my PhD at <a
                                                href="https://www.univ-grenoble-alpes.fr">University of Grenoble</a>!
                                        </li>
                                        <li>
                                            [<em>2020-09</em>] -
                                            <a href="#row-20mochi">MoCHi</a> is accepted to <a
                                                href="https://nips.cc/Conferences/2020">NeurIPS 2020</a>!
                                        </li>
                                        <li>
                                            [<em>2020-08</em>] -
                                            <a href="#row-20icmlm">ICMLM</a> is accepted to <a
                                                href="https://eccv2020.eu">ECCV 2020</a>!
                                            Check out our demo <a
                                                href="https://icmlm.europe.naverlabs.com/public">here</a> (it is very
                                            cool!).
                                        </li>
                                        <li>
                                            [<em>2020-08</em>] -
                                            <a href="#row-20keyp">"Key protected classification for collaborative
                                                learning"</a> is accepted to <a
                                                href="https://www.sciencedirect.com/science/article/abs/pii/S0031320320301308">Pattern
                                                Recognition</a>!
                                        </li>
                                        <li> [<em>2019-09</em>] - I Joined NAVER LABS Europe as a researcher. </li>
                                        <li> [<em>2019-09</em>] - I defended my master thesis, yey! </li>
                                        <li>
                                            [<em>2019-05</em>] -
                                            <a href="#row-19gmn">GMN</a> is accepted to <a
                                                href="https://cvpr2019.thecvf.com/">CVPR 2019</a> (oral presentation)!
                                        </li>
                                    </ul>
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <!-- Research -->
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Research</heading>
                                    <p>
                                        I'm interested in computer vision, more specifically, self-supervised learning
                                        (SSL) and vision & language (VL).
                                        In SSL, the goal is to learn general-purpose visual representations (prior
                                        knowledge about visual world) without relying on human-defined semantic
                                        annotations.
                                        I try to understand what makes general-purpose representations and how to learn
                                        them with only unlabeled data.
                                        VL, on the other hand, tries to align or complement visual and accompanying
                                        textual data representations.
                                        I find it exciting to study how language can provide prior knowledge to learn
                                        visual representations that can extrapolate to unseen visual concepts.
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <!-- Publications -->
                    <table
                        style="width:100%;border:0px;border-spacing:10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>

                            <tr id="row-22trex" class="pubrow">
                                <td class="pubimagecol">
                                    <img src="pub/2022-trex/in1k_vs_transfer.png" alt="in1k vs transfer plane"
                                        style="width:100%;height:auto">
                                </td>
                                <td class="pubpapercol">
                                    <a href="https://arxiv.org/abs/2206.15369">
                                        <papertitle>Improving the Generalization of Supervised Models</papertitle>
                                    </a>
                                    <br>
                                    Mert Bulent Sariyildiz, <a href="https://www.skamalas.com">Yannis Kalantidis</a>, <a
                                        href="https://thoth.inrialpes.fr/people/alahari">Karteek Alahari</a>, <a
                                        href="https://dlarlus.github.io">Diane Larlus</a>
                                    <br>
                                    <p>
                                        We revisit supervised learning on ImageNet-1K and propose a training setup which
                                        improves transfer learning performance of supervised models.<br>
                                        <a href="https://europe.naverlabs.com/t-rex">project website</a>
                                    </p>
                                </td>
                            </tr>

                            <tr id="row-21cog" class="pubrow">
                                <td class="pubimagecol">
                                    <img src="pub/2021-cog/cog-levels.png" alt="hypothetical-cog-levels"
                                        style="width:100%;height:auto">
                                </td>
                                <td class="pubpapercol">
                                    <a href="https://arxiv.org/abs/2012.05649">
                                        <papertitle>ImageNet-CoG | Concept Generalization in Visual Representation
                                            Learning</papertitle>
                                    </a>
                                    <br>
                                    Mert Bulent Sariyildiz, <a href="https://www.skamalas.com">Yannis Kalantidis</a>, <a
                                        href="https://dlarlus.github.io">Diane Larlus</a>, <a
                                        href="https://thoth.inrialpes.fr/people/alahari">Karteek Alahari</a>
                                    <br>
                                    <em><a href="https://iccv2021.thecvf.com/home">ICCV 2021</a></em>
                                    <br>
                                    <p>
                                        We propose a benchmark tailored for measuring concept generalization
                                        capabilities of models.<br>
                                        <a href="https://europe.naverlabs.com/cog-benchmark">project website</a>,
                                        <a href="https://github.com/naver/cog">code</a>,
                                        <a href="pub/2021-cog/ICCV2021_ImageNet-CoG_poster.pdf">poster</a>,
                                        <a href="pub/2021-cog/ICCV2021_ImageNet-CoG_presentation.pdf">presentation
                                            (PDF)</a>,
                                        <a href="pub/2021-cog/ICCV2021_ImageNet-CoG_presentation.pptx">presentation
                                            (PPT)</a>,
                                        <a href="https://youtu.be/1pMkHaETA6U">video</a>
                                    </p>
                                </td>
                            </tr>

                            <tr id="row-20mochi" class="pubrow">
                                <td class="pubimagecol">
                                    <img src="pub/2020-mochi/mochi-mixing-negatives.png" alt="mochi-mixing-negatives"
                                        style="width:100%;height:auto">
                                </td>
                                <td class="pubpapercol">
                                    <a href="https://arxiv.org/abs/2010.01028">
                                        <papertitle>MoCHi | Hard Negative Mixing for Contrastive Learning</papertitle>
                                    </a>
                                    <br>
                                    <a href="https://www.skamalas.com">Yannis Kalantidis</a>,
                                    Mert Bulent Sariyildiz,
                                    <a href="https://fr.linkedin.com/in/noe-pion">Noe Pion</a>,
                                    <a href="https://europe.naverlabs.com/people_user/philippe-weinzaepfel">Philippe
                                        Weinzaepfel</a>,
                                    <a href="https://dlarlus.github.io">Diane Larlus</a>
                                    <br>
                                    <em><a href="https://nips.cc/Conferences/2020/">NeurIPS 2020</a></em>
                                    <p>
                                        For contrastive learning, sampling more or harder negatives often improve
                                        performance.
                                        We propose two ways to synthesize more negatives using the MoCo framework.<br>
                                        <a href="https://europe.naverlabs.com/research/computer-vision/mochi">project
                                            website</a>
                                    </p>
                                </td>
                            </tr>

                            <tr id="row-20icmlm" class="pubrow">
                                <td class="pubimagecol">
                                    <img src="pub/2020-icmlm/masked-token-attention.gif"
                                        alt="icmlm-masked-token-attention" style="width:100%;height:auto">
                                </td>
                                <td class="pubpapercol">
                                    <a href="https://arxiv.org/abs/2008.01392">
                                        <papertitle>ICMLM | Learning Visual Representations with Caption Annotations
                                        </papertitle>
                                    </a>
                                    <br>
                                    Mert Bulent Sariyildiz,
                                    <a href="https://europe.naverlabs.com/people_user/Julien-Perez">Julien Perez</a>,
                                    <a href="https://dlarlus.github.io">Diane Larlus</a>
                                    <br>
                                    <em><a href="https://eccv2020.eu/">ECCV 2020</a></em>
                                    <p>
                                        Images often come with accompanying text describing the scene in images.
                                        We propose a method to learn visual representations using (image, caption)
                                        pairs.<br>
                                        <a href="https://europe.naverlabs.com/research/computer-vision/icmlm/">project
                                            website</a>,
                                        <a href="https://icmlm.europe.naverlabs.com/public">demo</a>
                                    </p>
                                </td>
                            </tr>

                            <tr id="row-20keyp" class="pubrow">
                                <td class="pubimagecol">
                                    <img src="pub/2020-keyp/keyp-compute-chain.png" alt="keyp-compute-chain"
                                        style="width:100%;height:auto">
                                </td>
                                <td class="pubpapercol">
                                    <a href="https://arxiv.org/abs/1908.10172">
                                        <papertitle>Key protected classification for collaborative learning</papertitle>
                                    </a>
                                    <br>
                                    Mert Bulent Sariyildiz,
                                    <a href="https://user.ceng.metu.edu.tr/~gcinbis">Ramazan Gokberk Cinbis</a>,
                                    <a href="https://case.edu/issacs/faculty-associates/erman-ayday">Erman Ayday</a>
                                    <br>
                                    <em><a href="https://www.journals.elsevier.com/pattern-recognition">Pattern
                                            Recognition</a></em>
                                    <p>
                                        Vanilla collaborative learning frameworks are vulnerable to an active adversary
                                        that runs a generative adversarial network attack.
                                        We propose a classification model that is resilient against such attacks by
                                        design.<br>
                                        <a href="https://github.com/mbsariyildiz/key-protected-classification">code
                                            repo</a>
                                    </p>
                                </td>
                            </tr>

                            <tr id="row-19gmn" class="pubrow">
                                <td class="pubimagecol">
                                    <img src="pub/2019-gmn/gmn-front.png" alt="gmn-front-figure"
                                        style="width:100%;height:auto">
                                </td>
                                <td class="pubpapercol">
                                    <a
                                        href="https://openaccess.thecvf.com/content_CVPR_2019/html/Sariyildiz_Gradient_Matching_Generative_Networks_for_Zero-Shot_Learning_CVPR_2019_paper.html">
                                        <papertitle>GMN | Gradient Matching Generative Networks for Zero-Shot Learning
                                        </papertitle>
                                    </a>
                                    <br>
                                    Mert Bulent Sariyildiz,
                                    <a href="https://user.ceng.metu.edu.tr/~gcinbis">Ramazan Gokberk Cinbis</a>
                                    <br>
                                    <em><a href="https://cvpr2019.thecvf.com/">CVPR 2019</a>, oral presentation</em>
                                    <p>
                                        Zero-shot learning models may suffer from the domain-shift due to the difference
                                        between data distributions of seen and unseen concepts.
                                        We propose a generative model to synthesize samples for unseen concepts given
                                        their visual attributes and use these samples for training a classifier for both
                                        seen and unseen concepts.<br>
                                        <a href="https://github.com/mbsariyildiz/gmn-zsl">code repo</a>
                                    </p>
                                </td>
                            </tr>

                        </tbody>
                    </table>

                    <!-- Community service -->
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Community Service</heading>
                                    <p>

                                    <ul>
                                        <li><a href="">Reviewer for ECCV 2022</a></li>
                                        <li><a href="https://cvpr2022.thecvf.com/all-reviewers">Reviewer for CVPR
                                                2022</a></li>
                                        <li><a href="">Reviewer for ICCV 2021</a></li>
                                        <li><a href="http://cvpr2021.thecvf.com/node/181">Reviewer for CVPR 2021</a>
                                        </li>
                                        <li><a href="https://nips.cc/Conferences/2020/Reviewers">Reviewer for NeurIPS
                                                2020</a></li>
                                    </ul>

                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <!-- Website notice -->
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:0px">
                                    <p style="text-align:right;font-size:small;">
                                        Huge thanks to <a href="https://jonbarron.info/">Jon Barron</a>, who provides
                                        the template of this website.
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                </td>
            </tr>
    </table>
</body>

</html>