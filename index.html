<!DOCTYPE HTML>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Mert Bulent Sariyildiz's Personal Website</title>

    <meta name="author" content="Mert Bulent Sariyildiz">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/png" href="profile/website-icon.png">
</head>

<body>
    <table
        style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">

                    <!-- BIO -->
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr style="padding:0px">
                                <td style="padding:2.5%;width:75%;vertical-align:middle">
                                    <p style="text-align:center">
                                        <name>Mert Bulent Sariyildiz</name>
                                    </p>
                                    <p style="text-align:center">
                                        Research Scientist at <a href="https://europe.naverlabs.com">NAVER LABS Europe</a>
                                    </p>
                                    <p>
                                        I was a doctoral researcher at <a href="https://europe.naverlabs.com">NAVER LABS Europe</a>
                                        and Inria Grenoble (<a href="https://team.inria.fr/thoth">the THOTH Team</a>) in France,
                                        under the supervision of
                                        <a href="https://www.skamalas.com">Yannis Kalantidis</a>,
                                        <a href="https://dlarlus.github.io">Diane Larlus</a> and
                                        <a href="https://thoth.inrialpes.fr/people/alahari">Karteek Alahari</a>.
                                        My PhD focused on learning general-purpose visual representations from images.
                                    </p>
                                    <p>
                                        I received my M.Sc. degree from <a href="https://w3.cs.bilkent.edu.tr/en">the
                                            Computer Engineering Department</a> at Bilkent University in Türkiye,
                                            where I worked with <a
                                            href="https://user.ceng.metu.edu.tr/~gcinbis">Gokberk Cinbis</a> on learning
                                        data-efficient visual classification models.
                                        Before that, I received my B.Sc. from <a href="https://eem.eskisehir.edu.tr">the Electrical
                                            and Electronics Engineering Department</a> at Anadolu (now Eskisehir
                                        Technical) University in Türkiye.
                                    </p>
                                    <p style="text-align:center">
                                        <a href="mailto:mbsariyildiz@gmail.com">Email</a> &nbsp/&nbsp
                                        <a href="data/MertBulentSariyildiz_resume.pdf">CV</a> &nbsp/&nbsp
                                        <a href="https://scholar.google.com/citations?user=9vpQ9tIAAAAJ&hl=en">Google
                                            Scholar</a> &nbsp/&nbsp
                                        <a href="https://twitter.com/mbsariyildiz">Twitter</a> &nbsp/&nbsp
                                        <a href="https://github.com/mbsariyildiz">Github</a>
                                    </p>
                                </td>
                                <td style="padding:2.5%;width:25%;max-width:25%">
                                    <img style="max-width:100%;height:auto;" alt="profile photo"
                                        src="profile/MertBulentSariyildiz-pic.png">
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <!-- News-->
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>News</heading>
                                    <p>
                                    <ul>
                                        <li>
                                            [<em>2023-07</em>] -
                                            Joined <a href="https://europe.naverlabs.com/"> NAVER LABS Europe</a> as a research scientist!
                                        </li>
                                        <li>
                                            [<em>2023-06</em>] -
                                            Successfully defended my PhD thesis!
                                        </li>
                                        <li>
                                            [<em>2023-02</em>] -
                                            <a href="#row-22sd">ImageNet-SD</a> is accepted to <a href="https://cvpr2023.thecvf.com/">CVPR 2023</a>!<br>
                                        </li>
                                        <li>
                                            [<em>2023-01</em>] -
                                            <a href="#row-22trex">t-ReX</a> is accepted to <a href="https://openreview.net/forum?id=3Y5Uhf5KgGK">ICLR 2023</a> as a spotlight presentation!<br>
                                        </li>
                                        <li>
                                            [<em>2022-12</em>] -
                                            New pre-print on arXiv: <a href="https://arxiv.org/abs/2212.08420">Fake it till you make it: Learning(s) from a synthetic ImageNet clone</a>
                                        </li>
                                        <li>
                                            [<em>2022-10</em>] -
                                            I'm selected as an <a href="https://eccv2022.ecva.net/program/outstanding-reviewers">outstanding
                                                reviewer at ECCV 2022</a>.
                                        </li>
                                        <li>
                                            [<em>2022-06</em>] -
                                            Our work <a href="#row-22trex">Improving the Generalization of
                                                Supervised Models</a> is on <a
                                                href="https://arxiv.org/abs/2206.15369">ArXiv</a>.<br>
                                            Pretrained weights for t-ReX and t-ReX* are available on <a
                                                href="https://europe.naverlabs.com/research/computer-vision/improving-the-generalization-of-supervised-models/#pretrained_models">our
                                                project
                                                webpage</a>.
                                        </li>
                                        <li>
                                            [<em>2021-08</em>] -
                                            <a href="#row-21cog">The ImageNet-CoG benchmark</a> is accepted to <a
                                                href="http://iccv2021.thecvf.com/home">ICCV 2021</a>!<br>
                                            We have a newer version of the manuscript, and our code is out!
                                        </li>
                                        <li>
                                            [<em>2021-07</em>] -
                                            I'm selected as an <a href="http://cvpr2021.thecvf.com/node/184">outstanding
                                                reviewer at CVPR 2021</a>.
                                        </li>
                                        <li>
                                            [<em>2020-12</em>] -
                                            The pre-print of the ImageNet-CoG Benchmark is on <a
                                                href="https://arxiv.org/abs/2012.05649">arXiv</a>.
                                        </li>
                                        <li>
                                            [<em>2020-09</em>] -
                                            I officially started my PhD at <a
                                                href="https://www.univ-grenoble-alpes.fr">University of Grenoble</a>!
                                        </li>
                                        <li>
                                            [<em>2020-09</em>] -
                                            <a href="#row-20mochi">MoCHi</a> is accepted to <a
                                                href="https://nips.cc/Conferences/2020">NeurIPS 2020</a>!
                                        </li>
                                        <li>
                                            [<em>2020-08</em>] -
                                            <a href="#row-20icmlm">ICMLM</a> is accepted to <a
                                                href="https://eccv2020.eu">ECCV 2020</a>!
                                            Check out our demo <a
                                                href="https://icmlm.europe.naverlabs.com/public">here</a> (it is very
                                            cool!).
                                        </li>
                                        <li>
                                            [<em>2020-08</em>] -
                                            <a href="#row-20keyp">"Key protected classification for collaborative
                                                learning"</a> is accepted to <a
                                                href="https://www.sciencedirect.com/science/article/abs/pii/S0031320320301308">Pattern
                                                Recognition</a>!
                                        </li>
                                        <li> [<em>2019-09</em>] - I Joined NAVER LABS Europe as a researcher. </li>
                                        <li> [<em>2019-09</em>] - I defended my master thesis, yey! </li>
                                        <li>
                                            [<em>2019-05</em>] -
                                            <a href="#row-19gmn">GMN</a> is accepted to <a
                                                href="https://cvpr2019.thecvf.com/">CVPR 2019</a> (oral presentation)!
                                        </li>
                                    </ul>
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <!-- Research -->
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Research</heading>
                                    <p>
                                        I'm broadly interested in computer vision problems.
                                        If I have to be more specific, I like working on learning visual representations from imagery data with different forms of supervision (including no supervision at all!) so that they are useful for a range of vision tasks.
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <!-- Publications -->
                    <table
                        style="width:100%;border:0px;border-spacing:10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>

                            <tr id="row-22sd" class="pubrow">
                                <td class="pubimagecol">
                                    <!-- Teaser figure -->
                                    <a href="pub/2022-sd/overview_generated.png" target="_blank">
                                    <img id="myImg" src="pub/2022-sd/model_overview.png"
                                        alt="ImageNet-SD model overview"
                                        style="width:100%;height:auto">
                                    </a>
                                </td>
                                <td class="pubpapercol">
                                    <a href="https://arxiv.org/abs/2212.08420">
                                        <papertitle> ImageNet-SD | Fake it till you make it: Learning(s) from a synthetic ImageNet clone</papertitle>
                                    </a>
                                    <br>
                                    Mert Bulent Sariyildiz, Karteek Alahari, Diane Larlus and Yannis Kalantidis
                                    <br>
                                    <em><a href="https://cvpr2023.thecvf.com/">CVPR 2023</a></em>
                                    <br>
                                    <p>
                                        Recent text-to-image generative models, generate fairly realistic images.
                                        <em>Could such models render real images obsolete for training image prediction models?</em>
                                        We answer part of this provocative question by questioning the need for real images when training models for ImageNet-1K classification.
                                        We show that models trained on synthetic images exhibit strong generalization properties and perform on par with models trained on real data.
                                    </p>
                                </td>
                            </tr>

                            <tr id="row-22trex" class="pubrow">
                                <td class="pubimagecol">
                                    <a href="pub/2022-trex/in1k_vs_transfer.png" target="_blank">
                                    <img id="myImg" src="pub/2022-trex/in1k_vs_transfer.png"
                                        alt="ImageNet-1K vs. transfer performance plane"
                                        style="width:100%;height:auto">
                                    </a>
                                </td>
                                <td class="pubpapercol">
                                    <a href="https://arxiv.org/abs/2206.15369">
                                        <papertitle> t-ReX | No reason for no supervision: Improving the generalization of supervised models</papertitle>
                                    </a>
                                    <br>
                                    Mert Bulent Sariyildiz, Yannis Kalantidis, Karteek Alahari and Diane Larlus
                                    <br>
                                    <em><a href="https://iclr.cc/Conferences/2023">ICLR 2023</a></em>
                                    <br>
                                    <p>
                                        We revisit supervised learning on ImageNet-1K and propose a training setup which
                                        improves transfer learning performance of supervised models.<br>
                                        <a href="https://europe.naverlabs.com/t-rex">project website</a>,
                                        <a href="https://openreview.net/forum?id=3Y5Uhf5KgGK">OpenReview</a>
                                    </p>
                                </td>
                            </tr>

                            <tr id="row-21cog" class="pubrow">
                                <td class="pubimagecol">
                                    <a href="pub/2021-cog/cog-levels.png" target="_blank">
                                    <img id="myImg" src="pub/2021-cog/cog-levels.png"
                                        alt="Hypothetical ImageNet-CoG levels"
                                        style="width:100%;height:auto">
                                    </a>
                                </td>
                                <td class="pubpapercol">
                                    <a href="https://arxiv.org/abs/2012.05649">
                                        <papertitle>ImageNet-CoG | Concept Generalization in Visual Representation
                                            Learning</papertitle>
                                    </a>
                                    <br>
                                    Mert Bulent Sariyildiz, Yannis Kalantidis, Diane Larlus and Karteek Alahari
                                    <br>
                                    <em><a href="https://iccv2021.thecvf.com/home">ICCV 2021</a></em>
                                    <br>
                                    <p>
                                        We propose a benchmark tailored for measuring concept generalization
                                        capabilities of models trained on ImageNet-1K.<br>
                                        <a href="https://europe.naverlabs.com/cog-benchmark">project website</a>,
                                        <a href="https://github.com/naver/cog">code</a>,
                                        <a href="pub/2021-cog/ICCV2021_ImageNet-CoG_poster.pdf">poster</a>,
                                        <a href="pub/2021-cog/ICCV2021_ImageNet-CoG_presentation.pdf">presentation
                                            (PDF)</a>,
                                        <a href="pub/2021-cog/ICCV2021_ImageNet-CoG_presentation.pptx">presentation
                                            (PPT)</a>,
                                        <a href="https://youtu.be/1pMkHaETA6U">video</a>
                                    </p>
                                </td>
                            </tr>

                            <tr id="row-20mochi" class="pubrow">
                                <td class="pubimagecol">
                                    <a href="pub/2020-mochi/mochi-mixing-negatives.png" target="_blank">
                                    <img id="myImg" src="pub/2020-mochi/mochi-mixing-negatives.png"
                                        alt="Mixing negatives in MoCHi"
                                        style="width:100%;height:auto">
                                    </a>
                                </td>
                                <td class="pubpapercol">
                                    <a href="https://arxiv.org/abs/2010.01028">
                                        <papertitle>MoCHi | Hard Negative Mixing for Contrastive Learning</papertitle>
                                    </a>
                                    <br>
                                    Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel and Diane Larlus
                                    <br>
                                    <em><a href="https://nips.cc/Conferences/2020/">NeurIPS 2020</a></em>
                                    <p>
                                        For contrastive learning, sampling more or harder negatives often improve
                                        performance.
                                        We propose two ways to synthesize more negatives using the MoCo framework.<br>
                                        <a href="https://europe.naverlabs.com/research/computer-vision/mochi">project
                                            website</a>
                                    </p>
                                </td>
                            </tr>

                            <tr id="row-20icmlm" class="pubrow">
                                <td class="pubimagecol">
                                    <a href="pub/2020-icmlm/masked-token-attention.gif" target="_blank">
                                    <img id="myImg" src="pub/2020-icmlm/masked-token-attention.gif"
                                        alt="Masked token attention in ICMLM"
                                        style="width:100%;height:auto">
                                    </a>
                                </td>
                                <td class="pubpapercol">
                                    <a href="https://arxiv.org/abs/2008.01392">
                                        <papertitle>ICMLM | Learning Visual Representations with Caption Annotations
                                        </papertitle>
                                    </a>
                                    <br>
                                    Mert Bulent Sariyildiz, Julien Perez and Diane Larlus
                                    <br>
                                    <em><a href="https://eccv2020.eu/">ECCV 2020</a></em>
                                    <p>
                                        Images often come with accompanying text describing the scene in images.
                                        We propose a method to learn visual representations using (image, caption)
                                        pairs.<br>
                                        <a href="https://europe.naverlabs.com/research/computer-vision/icmlm/">project
                                            website</a>,
                                        <a href="https://icmlm.europe.naverlabs.com/public">demo</a>
                                    </p>
                                </td>
                            </tr>

                            <tr id="row-20keyp" class="pubrow">
                                <td class="pubimagecol">
                                    <img src="pub/2020-keyp/keyp-compute-chain.png" alt="keyp-compute-chain"
                                        style="width:100%;height:auto">
                                </td>
                                <td class="pubpapercol">
                                    <a href="https://arxiv.org/abs/1908.10172">
                                        <papertitle>Key protected classification for collaborative learning</papertitle>
                                    </a>
                                    <br>
                                    Mert Bulent Sariyildiz, Ramazan Gokberk Cinbis and Erman Ayday
                                    <br>
                                    <em><a
                                            href="https://www.sciencedirect.com/journal/pattern-recognition/vol/104/suppl/C">Pattern
                                            Recognition, Vol. 104, August 2020</a></em>
                                    <p>
                                        Vanilla collaborative learning frameworks are vulnerable to an active adversary
                                        that runs a generative adversarial network attack.
                                        We propose a classification model that is resilient against such attacks by
                                        design.<br>
                                        <a href="https://github.com/mbsariyildiz/key-protected-classification">code
                                            repo</a>
                                    </p>
                                </td>
                            </tr>

                            <tr id="row-19gmn" class="pubrow">
                                <td class="pubimagecol">
                                    <img src="pub/2019-gmn/gmn-front.png" alt="gmn-front-figure"
                                        style="width:100%;height:auto">
                                </td>
                                <td class="pubpapercol">
                                    <a
                                        href="https://openaccess.thecvf.com/content_CVPR_2019/html/Sariyildiz_Gradient_Matching_Generative_Networks_for_Zero-Shot_Learning_CVPR_2019_paper.html">
                                        <papertitle>GMN | Gradient Matching Generative Networks for Zero-Shot Learning
                                        </papertitle>
                                    </a>
                                    <br>
                                    Mert Bulent Sariyildiz and Ramazan Gokberk Cinbis
                                    <br>
                                    <em><a href="https://cvpr2019.thecvf.com/">CVPR 2019</a>, oral presentation</em>
                                    <p>
                                        Zero-shot learning models may suffer from the domain-shift due to the difference
                                        between data distributions of seen and unseen concepts.
                                        We propose a generative model to synthesize samples for unseen concepts given
                                        their visual attributes and use these samples for training a classifier for both
                                        seen and unseen concepts.<br>
                                        <a href="https://github.com/mbsariyildiz/gmn-zsl">code repo</a>
                                    </p>
                                </td>
                            </tr>

                        </tbody>
                    </table>

                    <!-- Community service -->
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Community Service</heading>
                                    <p>

                                    <ul>
                                        <li><a href="">Reviewer for ECCV 2022</a></li>
                                        <li><a href="https://cvpr2022.thecvf.com/all-reviewers">Reviewer for CVPR
                                                2022</a></li>
                                        <li><a href="">Reviewer for ICCV 2021</a></li>
                                        <li><a href="http://cvpr2021.thecvf.com/node/181">Reviewer for CVPR 2021</a>
                                        </li>
                                        <li><a href="https://nips.cc/Conferences/2020/Reviewers">Reviewer for NeurIPS
                                                2020</a></li>
                                    </ul>

                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <!-- Website notice -->
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:0px">
                                    <p style="text-align:right;font-size:small;">
                                        Huge thanks to <a href="https://jonbarron.info/">Jon Barron</a>, who provides
                                        the template of this website.
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                </td>
            </tr>
    </table>
</body>

</html>